# Literature

If I want to read the fewest number of papers to catch up, I would
read (1) NUTS, and (2) ADVI.

Currently, the lowest hanging fruits to implement would be:
- SGLD
- ADVI

# Papers

- [HMC paper][1]
- [HMC NUTS][12]
- [HMC Concept][2]
- [Stochastic Gradient HMC][3]
- [Stochastic Gradient with Langevin Dynamics (SGLD)][11]
- [pytorch][4]
- [Variational Inference][5]
- [ADVI][6]
- [STAN][7]
    - [Documentation][8]
    - [User guide][9]
    - [Functions reference][10]
- [Stochastic Gradient Descent as Approximate Bayesian Inference][13]
- [AutoGrad][14]
- [Scorch - a scala autograd library][15]

[1]: https://arxiv.org/pdf/1206.1901.pdf
[2]: https://arxiv.org/pdf/1701.02434.pdf
[3]: http://proceedings.mlr.press/v32/cheni14.pdf
[4]: https://pytorch.org/
[5]: https://arxiv.org/pdf/1601.00670.pdf
[6]: http://www.jmlr.org/papers/volume18/16-107/16-107.pdf
[7]: https://mc-stan.org/
[8]: https://mc-stan.org/users/documentation/
[9]: https://mc-stan.org/docs/2_18/stan-users-guide/index.html
[10]: https://mc-stan.org/docs/2_18/functions-reference/index.html#overview
[11]: http://people.ee.duke.edu/~lcarin/398_icmlpaper.pdf
[12]: http://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf
[13]: https://arxiv.org/pdf/1704.04289.pdf
[14]: https://arxiv.org/pdf/1502.05767.pdf
[15]: https://github.com/botkop/scorch
